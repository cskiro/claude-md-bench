# claude-md-bench

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

CLI tool for benchmarking and optimizing CLAUDE.md files used by AI coding assistants.

## Overview

`claude-md-bench` analyzes and compares CLAUDE.md configuration files to help you create more effective instructions for AI coding assistants like Claude Code. It uses local LLM inference via Ollama to evaluate files on multiple dimensions:

- **Clarity**: Are instructions clear and specific?
- **Completeness**: Does it cover all essential areas?
- **Actionability**: Are there concrete, executable guidelines?
- **Standards**: Does it enforce quality standards (TDD, types, testing)?
- **Context**: Is there adequate project context and structure?

## Installation

### Prerequisites

1. **Python 3.11+**
2. **Ollama** - Local LLM inference
   ```bash
   # macOS
   brew install ollama

   # Start Ollama server
   ollama serve

   # Pull a model (llama3.2 is lightweight, qwen2.5:32b is better quality)
   ollama pull llama3.2:latest
   ```

### Install from source

```bash
git clone https://github.com/cskiro/claude-md-bench.git
cd claude-md-bench
pip install -e ".[dev]"
```

## Usage

### Check Ollama connectivity

```bash
# Verify Ollama is running and list available models
claude-md-bench check

# Check if specific model is available
claude-md-bench check --model qwen2.5:32b
```

### Compare two CLAUDE.md files

```bash
# Basic comparison
claude-md-bench compare ~/.claude/CLAUDE.md ~/project/CLAUDE.md

# With custom names and output directory
claude-md-bench compare file1.md file2.md \
  --name-a "Production" \
  --name-b "Development" \
  --output-dir ./reports

# Use a different model
claude-md-bench compare file1.md file2.md --model qwen2.5:32b

# Text-only output (no HTML)
claude-md-bench compare file1.md file2.md --format text
```

### Output

The tool generates:
- Console output with rich formatting
- Text report saved to `~/.claude-md-bench/reports/`
- HTML report with visual dimension comparison

## Example Output

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ  CLAUDE.md Comparison Results          â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Version             â”‚ Score     â”‚ Size       â”‚ Winner â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ A: Production       â”‚ 85.0/100  â”‚ 15,234     â”‚        â”‚
â”‚ B: Development      â”‚ 72.0/100  â”‚ 8,567      â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ† Winner: Version A (+13.0 points)

Dimension Scores
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dimension       â”‚ Version A â”‚ Version B â”‚ Delta â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Clarity         â”‚ 85        â”‚ 70        â”‚ +15   â”‚
â”‚ Completeness    â”‚ 80        â”‚ 65        â”‚ +15   â”‚
â”‚ Actionability   â”‚ 90        â”‚ 75        â”‚ +15   â”‚
â”‚ Standards       â”‚ 85        â”‚ 80        â”‚ +5    â”‚
â”‚ Context         â”‚ 85        â”‚ 70        â”‚ +15   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development

### Setup development environment

```bash
# Clone repository
git clone https://github.com/cskiro/claude-md-bench.git
cd claude-md-bench

# Install with dev dependencies
pip install -e ".[dev]"
```

### Run tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=claude_md_bench --cov-report=html

# Run specific test file
pytest tests/test_analyzer.py -v
```

### Code quality

```bash
# Lint with ruff
ruff check src tests

# Type check with mypy
mypy src

# Format code
ruff format src tests
```

## Architecture

```
src/claude_md_bench/
â”œâ”€â”€ cli.py              # Main Typer application
â”œâ”€â”€ commands/
â”‚   â””â”€â”€ compare.py      # Compare command
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ analyzer.py     # CLAUDE.md analysis logic
â”‚   â””â”€â”€ reporter.py     # Report generation
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ ollama.py       # Ollama client wrapper
â””â”€â”€ templates/
    â””â”€â”€ comparison.html # HTML report template
```

## Roadmap

### Phase 1 (Current): Compare âœ…
- A/B comparison of CLAUDE.md files
- Multi-dimensional scoring
- HTML and text reports

### Phase 2: Benchmark
- Execute tasks against CLAUDE.md configurations
- TDD compliance evaluation
- Performance metrics

### Phase 3: Optimize
- Meta-prompting for CLAUDE.md improvement
- Iterative optimization loops
- Automated recommendations

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- Built with [Typer](https://typer.tiangolo.com/) for the CLI
- Uses [Rich](https://rich.readthedocs.io/) for beautiful console output
- Powered by [Ollama](https://ollama.ai/) for local LLM inference
- Inspired by Arize AI's prompt optimization research
